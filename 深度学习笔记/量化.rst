量化
==========

**PTQ** ：post-training quantization

截断量化和非截断量化
-----------------------------
.. figure:: /images/深度学习/非截断量化.png
    :align: center
    :width: 480

    非截断量化

.. figure:: /images/深度学习/截断量化.png
    :align: center
    :width: 480

    截断量化


对称量化和非对称量化
---------------------------------
* 对称量化：以0为中心轴，量化到[-127,127]
* 非对称量化：量化范围为[0,255]

.. figure:: /images/深度学习/对称量化.png
    :align: center
    :width: 480

    对称量化

.. figure:: /images/深度学习/非对称量化.png
    :align: center
    :width: 480

    非对称量化


线性量化公式
-----------------------
量化： ``Q = R/S + Z``

反量化： ``R = (Q-Z)*S``

.. math:: 

    S = \frac{R_{max}-R_{min}}{Q_{max}-Q_{min}}

    Z = Q_{max} - \frac{R_{max}}{S}

* R:输入的浮点数
* Q:量化后的定点表示
* Z:零点(Zero Point)的数值
* S:缩放因子(scale)的数值
* :math:`R_{max}` 表示浮点数中的最大值
* :math:`R_{min}` 表示浮点数中的最小值
* :math:`Q_{max}` 表示定点数中的最大值(127/255)
* :math:`Q_{min}` 表示定点数中的最小值(-128/0)


ggml量化公式
------------------------
.. code-block:: python 

    def quantize_q8_0(tensor: torch.Tensor) -> torch.CharTensor:
        GGML_QK8_0 = 32
        # equivalent to ggml_quantize_q8_0 in ggml.c
        assert tensor.shape[1] % GGML_QK8_0 == 0
        tensor = tensor.view(-1, GGML_QK8_0)
        scale = tensor.abs().max(dim=-1, keepdim=True).values / ((1 << 7) - 1)
        tensor = (tensor / scale).round().clamp(min=-128, max=127).char()
        # add scale into each block
        tensor = torch.cat((scale.half().view(torch.int8), tensor), dim=-1)
        return tensor


量化部署
----------------

* fp32输入，fp32输出
* fp32输入，int8输出
* int8输入，fp32输出

.. figure:: /images/深度学习/量化部署.png
    :align: center
    :width: 720

    量化部署

反量化
```````````
将int32结果反量化为float32

.. image:: /images/深度学习/反量化.png
    :width: 480

重量化(requant)
`````````````````````
将int32结果反量化为float32

.. image:: /images/深度学习/重量化.png
    :width: 480


k-quant
------------------------



round-to-nearest(RTN)
-----------------------------------


smoothquant
---------------------
https://github.com/mit-han-lab/smoothquant



Layer-Wise Quantization
----------------------------------------


Optimal Brain Quantization(OBQ)
------------------------------------


ZeroQuant
-----------------------

https://arxiv.org/pdf/2206.01861.pdf

1. 权重和激活都会量化
2. LKD( layer-by-layer knowledge distillation)算法，不依赖原始的训练数据
3. 高度优化的量化系统后端支持，以消除quantization/dequantization的开销
4. ZeroQuant可以以无成本的方式将权重和激活的精度降低到INT8
5. 速度提升5.19x/4.16x
6. LKD算法将全连接层量化为INT4;Attention层和激活层量化为INT8;相比fp16内存占用减少3倍
7. 将quant和激活层进行了融合，将dequant层和GEMM层进行了融合
8. 只在bert和gpt上验证过效果

GPTQ
-----------------
https://github.com/IST-DASLab/gptq

1. 第一个将千亿参数的模型量化到3-4bit;之前的后训练量化方法只能保证千亿参数的8bit量化的精度
2. 效率比较高，可以在几个小时内将具有千亿参数模型压缩到3bit/4bit;精度不会有明显损失
3. 实现了定制GPU内核，OPT-175B在A100 GPU上量化后性能提升3.25倍，A6000 GPU上量化后性能提升4.5倍
4. 没有量化激活层


ZeroQuant-V2
----------------------------------



AWQ(Activation-aware Weight Quantization)
----------------------------------------------------
https://github.com/mit-han-lab/llm-awq

1. 4090上3.7倍速度提升


SqueezeLLM
------------------------
https://github.com/SqueezeAILab/SqueezeLLM

1. Sensitivity-based Non-Uniform Quantization 基于灵敏度的非均匀量化。
2. Dense-and-Sparse Quantization 密集-稀疏量化。
   将权重分解为密集和稀疏两部分。稀疏部分使用高效的稀疏存储方法全精度保存离群值，并利用高效的稀疏内核将推理开销降至最低。
   这使得密集部分的范围更加紧凑，并有助于量化。通过只提取0.45%的权重作为稀疏成分，将llama7b在C4上的可思量度从7.75降至7.58
3. 2.3倍速度提升