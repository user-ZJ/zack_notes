大模型推理框架
====================================

+-------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+
|    框架     |                                    地址                                     |                                             说明                                             |
+=============+=============================================================================+==============================================================================================+
| llama.cpp   | https://github.com/ggerganov/llama.cpp                                      | 边加云，支持CPU和GPU，但只有encoder层运行在GPU上，embed、head、norm层仍运行在CPU上           |
+-------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+
| llama.onnx  | https://github.com/tpoisonooo/llama.onnx                                    | 导出的onnx在cpu上正常运行，                                                                  |
|             |                                                                             | 但onnx转tensorrt后数据不一致：https://github.com/NVIDIA/TensorRT/issues/2928                 |
|             |                                                                             | chatglm实现https://github.com/vllm-project/vllm/pull/649                                     |
+-------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+
| ChatGLM-MNN | https://github.com/wangzhaode/ChatGLM-MNN                                   | 一个基于 MNN 的 ChatGLM-6B C++ 推理实现，支持根据显存大小自动分配计算任务给 GPU 和 CPU       |
+-------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+
| deepspeed   | https://www.deepspeed.ai/tutorials/inference-tutorial/                      |                                                                                              |
+-------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+
| lyraChatGLM | https://huggingface.co/TMElyralab/lyraChatGLM                               | 对 ChatGLM-6B 进行推理加速，最高可以实现 9000+ tokens/s 的推理速度                           |
+-------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+
| JittorLLMs  | https://github.com/Jittor/JittorLLMs                                        | 目前支持了4种大模型：ChatGLM大模型；鹏程盘古大模型；BlinkDL的ChatRWKV；国外Meta的LLaMA大模型 |
+-------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+
| InferLLM    | https://github.com/MegEngine/InferLLM                                       | 基于llama.cpp修改版本，只支持CPU                                                             |
+-------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+
| vllm        | https://github.com/vllm-project/vllm                                        | https://github.com/vllm-project/vllm/discussions/275                                         |
+-------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+
| chatglm.cpp | https://github.com/li-plus/chatglm.cpp                                      |                                                                                              |
+-------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+
| Megatron-LM | https://github.com/NVIDIA/Megatron-LM                                       | 支持张量并行                                                                                 |
+-------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+
| mlc-llm     | https://github.com/mlc-ai/mlc-llm                                           |                                                                                              |
+-------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+
| T-MAC       | https://github.com/microsoft/T-MAC                                          |                                                                                              |
+-------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+
| MNN         |                                                                             |                                                                                              |
+-------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+
| PowerInfer  | https://github.com/SJTU-IPADS/PowerInfer                                    | 云端计算                                                                                     |
+-------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+
| executorch  | https://github.com/pytorch/executorch                                       | 端侧                                                                                         |
+-------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+
| mediapipe   | https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference?hl=zh-cn | 端侧+云侧                                                                                    |
+-------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+
| sglang      |                                                                             | 云侧                                                                                         |
+-------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+
| OpenLLM     | https://github.com/bentoml/OpenLLM                                          | 云侧                                                                                         |
+-------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+
| fastllm     | https://github.com/ztxz16/fastllm                                           | 端侧+云侧                                                                                    |
+-------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+


https://mp.weixin.qq.com/s/xIbNSAI9cKGIA19yZhIEgg

https://mp.weixin.qq.com/s/e-3qkkAY3vpAnFlSNviotg

https://developer.volcengine.com/articles/7433674165921513523

https://kittenyang.com/llm-on-device/